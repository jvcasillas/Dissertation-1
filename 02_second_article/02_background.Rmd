# 1. Introduction
Learning a second language as adults is difficult.
We draw from our previous knowledge to make up for the shortcomings we have in our second language (L2).
The type of previous knowledge we resort to is varied, from other languages we speak to our musical training to learn L2 tones.
If, how, and to what extent skills are transferred in learning is a hot debate.
Transfer could take place between domain-general and domain-specific mechanisms and between domain-specific mechanisms alone.
Transfer of domain-specific mechanisms alone in regards to language has been widely tested with music, for example, in learning of tones in a second language [@tang2016musical] or speech processing [e.g., @dittinger2016professional; @fitzroy2013musical; @musacchia2007musicians].
In speech processing, anticipation of information plays an important role to ease comprehension and processing load.
Inability to predict oncoming information and cognitive overload may be a source of lack of comprehension in second language learners.
While speech prediction improves with language exposure [e.g., _@Primerexperimento_, @sagarra2018suprasegmental], the issue still remains how speech processing is affected by other cognitive capacities.

In this paper, we investigate how auditory abilities condition speech prediction in a first language (L1) and in an L2.
Specifically, we aimed to investigate how pitch and rhythm innate abilities condition prediction of verb tense suffixes in L1 and L2 speakers of Spanish when cued by lexically stressed and unstressed syllables.
To observe the interaction between innate auditory abilities with the L1 in acquiring and developing L2 processing skills, we tested speakers whose L1s have lexical stress but in which pitch is not preeminent as an acoustic correlate (English and Spanish) with speakers whose L1 does not have lexical stress but for whom pitch is important (Mandarin Chinese). 
In addition Spanish and Mandarin Chinese are similar in terms of rhythms as they are syllable-timed, while English is stress-timed.

# 2. Background
## 2.1. Music and speech
Research with musicians has been prolific in providing evidence in support of the link between speech and music.
Behavioral online studies and neurocognitive studies have revealed that musical abilities influence positively the production and perception of linguistic sound structures in monolingual adults [@wong2007musical] and L2 learners [@marques2007musicians; @slevc2006individual].
Similarly, neurocognitive studies reveal that musicians process speech in a more enhanced way and more synchronously to its onset than non-musicians [e.g., @dittinger2016professional; @fitzroy2013musical; @musacchia2007musicians]. 
In a meta-analysis, @kraus2010music concluded that music training can cause functional and structural changes in the brain.
These changes may translate into improvement in language performance as music shares with language physiological resources that process auditory information.
Given that music and language share physiological resources and that musical practice gives an advantage in speech processing and in L2 acquisition, the relationship between both domains may extend further than training.
In other words, varying innate abilities may be responsible for individual variation of L2 processing, in particular of phonology, among learners of L2s.
Since music is a broad term, here we will focus on pitch, rhythm and their relationship with speech processing in an L1 and an L2.

_Pitch_   
Pitch is the frequency associated to a sound wave; this frequency places the sound within a scale ranging from low to high in perception [@klapuri2006introduction]. 
Pitch in music plays a similar role to the one it plays in language. 
In language, a listener must process the melodic contour of speech, that is, the changes up and down of pitch in order to understand the meaning conveyed by prosody [@dilley2005phonetics].  
For example, a rising pitch contour signals a question (“Coffee?”), while a falling intonation signals a statement (“Coffee.”). 
Apart from the phrasal and sentential intonation, pitch also affects tones and predominance of a syllable against others within a word. 
In music, pitch affects the note we hear.    

Pitch is not only a common acoustic correlate in speech and music.
Knowledge and perception abilities of pitch in one of the two auditory domains translates in many cases into increased perception abilities in the other domain [@bidelman2013tone; @chan2019lexical], such that native speakers of tonal languages discriminate absolute pitch in music more easily [e.g., @chua2014effect; @deutsch2009absolute; @ngo2016effects; @tsukada2015perception].
Native tonal speakers, in fact, perform more similarly to musicians than to non-tonal non-musicians in most pitch and music perception tasks [@bidelman2013tone].
One exception to this association appears to be relative pitch, whose discrimination is not facilitated when through L1 tone knowledge [@ngo2016effects]. 
In turn, musicianship contributes to better lexical tone perception even in a tonal language [@tang2016musical].
Because of these results, some authors have suggested that music and language abilities transfer bidirectionally [@bidelman2013tone].

The transfer may also happen when an L2 is involved, although the effects may be weaker.
Tonal memory is significantly correlated to the skill to discriminate L2 vowels [@mokari2018perceptual], and musical knowledge contributes to L2 tone discrimination [@ngo2016effects].
However, musical knowledge does not contribute to the creation of L2 tone-segment connections when the L1 does not encode tones; only a tonal L1 promotes the creation of L2 tone-segment connections and of a rule-like system of L2 tone information [@chan2019lexical]. 
Likewise, musical ability in general is not associated with the ability to discriminate and produce L2 sounds [@mokari2018perceptual].

From this set of results, we can deduce that there is a limit in the reciprocal influence between music and language (musical and linguistic pitch for the purpose of this project), but where this limit lies has not been ascertained. 
The research on the relationship of speech sounds with music has been mostly conducted with tones and about discrimination.
Exploring other structures in speech that conform the prosody of a language, namely lexical stress and moras, could provide insight into the connection between pitch and speech.

_Rhythm_    
Rhythm is defined as a pattern of recurrent time intervals that usually occur periodically [@berlyne1971aesthetics].
This periodic nature allows to predict the start of an interval or the next recurring event based on what has already been perceived [@fraisse2013rhythm; @martin1972rhythmic]. 
A typical response to a rhythm is synchronized movement to it. 
This sensorimotor behavior is based on anticipation of when the next beat is coming, especially at slow tempi [@nozaradan2016individual; @van2015sensorimotor].
Ability to perform this anticipation grows in accuracy with years of musical training [@nozaradan2016individual]. 
Prediction of oncoming beats is also present in rhythms with tempo changes [@van2015sensorimotor].   
Within the auditory modality, music is probably the most obvious instance of rhythm, but other domains such as language also follow rhythmic patterns.
Metrical expectancies are automatically generated as we listen to speech [e.g., @magne2016speech; @rothermich2012rhythm; @schmidt2009event]. 
Speakers use stress patterns to generate the expectancies. 
The expectancies are created because the stress patterns are connected to other linguistic levels. 
For instance, stress can be associated with word endings [@soto2001], such that by hearing a specific stress patterns, the speaker can predict verb tense [@sagarra2018; @primerexperimento].
Speakers need to learn these associations.
External factors such as learning context [@primerexperimento] or verb frequency may contribute to the easiness with which the connections is created. 
However, internal factors may also contribute. 

Individual differences in music rhythm abilities can account for speech rhythm sensitivity [@magne2016].
The similarity of rhythmic processing in music and speech may involve shared cognitive mechanisms to perceive and process the rhythms in different domains that may be causing the individual differences.
If the mechanisms are shared, then increased abilities in rhythm, be them innate or acquired through practice, may have an effect on discrimination and processing of rhythms in language.
Indeed, rhythmic abilities and perception condition L1 and L2 processing. 
Matching metrical primes facilitate processing of sentences in the L1 [@cason2015bridging].
ERP recordings have shown that on-beat sequence primes result in faster L1 speech processing than off-beat primes in adults [@cason2012rhythmic]. Rhythmic production is positively associated with L2 lexical stress placement and rhythm perception with the L2 learning experience [@bhatara2015foreign; @cason2019rhythmic].
Better increased rhythmic perception, in contrast, is not associated with adults' reading abilities in an L1 or L2 [@swaminathan2018explaining].
Swaminathan and colleagues were, nevertheless, mixing modalities (written text and heard rhythms), while Cason and colleagues were not. 
We could therefore hypothesize that a relationship between speech and rhythm is still not precluded.
This conclusion would also agree with with association learning models proposing that the more similar two cognitive specific domains are, the likelier the transfer of abilities and mechanisms between them is.

In order to extend the current findings on the association between music and speech to lexical stress and predictive processing, we tested pitch, rhythm and speech anticipation abilities of monolingual and L2 speakers of Spanish.
There is evidence for the existence of prediction processes in music [e.g., @salimpoor2015predictions] as well as in language.
Anticipation in music can be cued through syntactic structure [@sammler2013syntax] or acoustic properties [@loui2007harmonic].
In language, there are many layers of prediction.
Importantly for this study, phonology can cue outcomes.

## 2.2. Linguistic anticipation
Anticipation alleviates the load of constantly processing the information that we receive from the world. 
At a large scale, we anticipate what is going to happen in movies and books. 
At smaller scales, we anticipate movements in sports, other drivers' actions, and we also anticipate what we read and what we hear. 
In linguistic anticipation, we use cues of different kinds to predict oncoming linguistic information. 
Predictive processing in language is especially efficient in the L1, and mediated by factors like L1 transfer, L2 proficiency, or working memory in the L2. 
These factors may interact with each other to generate successful or unsuccessful predictions.

Relevant for this study, predictions may be driven by phonological information.
Depending on the language, coarticulation [@salverda2014immediate], intonation [@nakamura2012immediate; @weber2006role], lexical stress [@correia2013word; @sagarra2018suprasegmental], pauses between clauses [@hawthorne2014pauses; @kjelgaard1999prosodic], vowel duration [@rehrig2017acoustic], and tone [@roll2015neurolinguistic; @roll2011activating] can be cues to outcomes.
The outcomes with which the cues are associated are varied.
In Swedish, tonal cues indicate noun number [if low tone, then singular, _fisken_ 'fish~[SG]~'; if high tone, then plural, _fiskar_ 'fish~[PL]~,' @roll2010word; @soderstrom2015using; @roll2013word] and verb tense [if low tone, then present, _skrämmer_ 'I scare'; if high tone, then past, _skrämde_ 'I scared,' @soderstrom2012processing; @roll2015neurolinguistic].
In Spanish, lexical stress signals verbal tense [if first syllable stressed, then present; if unstressed, then past: _CANta_ 'he sings' vs _canTÓ_ 'he sang,' @sagarra2018suprasegmental] and noun ending [*PRINcipe* 'prince' vs _prinCIPIO_ 'beginning,' @soto2001segmental]. In English, vowel duration is associated with voice [if shorter vowel duration, then active: 'the girl was pushing the boy'; if longer vowel duration, then passive: 'the girl was pushed by the boy,' @rehrig2017acoustic].  

L2 speakers require time to learn the associations between phonological information and the outcomes, and their learning may be conditioned by several factors, such as learning context or L1 knowledge.
In L2 settings, only L2 learners at advanced levels of proficiency can generate predictions based on phonological structures [@rehrig2017acoustic] and depending on syllable structure [@sagarra2018suprasegmental].
When immersed, speakers learn at lower proficiencies to make the associations that enable them to predict [@berthelsen2018neural; @PRIMEREXPERIMENTO; @hed2019neural; @schremm2016implicit].
Learning context,however, should also be considered in light of the L1.
The L1 and L2 of the speakers in @sagarra2018suprasegmental shared the use of lexical stress, while the languages of speakers in @rehrig2017acoustic did not share vowel lengthening, and still they got similar results.
In contrasts, the L2 speakers learning tones in @hed2019neural and @schremm2016implicit did not have tonal L1s and at intermediate stages could generate predictions, while the speakers not sharing structures in @PRIMEREXPERIMENTO (Mandarin Chinese speakers) were slower in learning the lexical stress associations than the population that did share the structure (English speakers).
The mixed results thus suggest that different factors interact in an L2 learner's ability to acquire cue-outcome mappings.

Among the factors considered in contributing to the association learning, factors regarding individual differences have been largely disregarded.
Working memory has been considered in some studies as an individual variability factor, and only two of those tested anticipation based on phonological structures [@lozano2020; @sagarra2018suprasegmental].
Both studies suggest that working memory plays no role in ability to anticipate either in the L1 or the L2 [@sagarra2018suprasegmental], unless the L2 speakers are extremely taxed cognitively [@lozano2020].
Given that working memory is not relevant for prediction, other individual characteristics that are associated with speech, e.g., auditory processing, may exert some influence.

In order to test the hypothesis that individual differences in the creation of L2 associations between phonological cues with outcomes for predictions stem from innate abilities in auditory processing, we investigate in this paper whether innate abilities for rhythm and pitch and practice with acoustic correlates in the L1 affect the creation of lexical stress cue-suffix outcome associations used in speech processing. 
In particular, we asked whether better pitch and rhythm abilities affect English and Mandarin learners of Spanish at different levels of proficiency and monolingual speakers of Spanish to predict verb tense suffixes that are cued by lexical stress.
We included L2 speakers at different levels of proficiency because the ability to use lexical stress to predict tense evolves along with exposure to the L2 [@sagarra2018suprasegmental; @PRIMER EXP].
In addition, it is possible that cross-domain transfer varies over proficiency, just like domain-general mechanisms such as working memory affect L2 learning differently at different stages [@serafini2016evidence].
We chose native English and Mandarin speakers because of the phonological and phonetic phenomena their L1 encodes and how they differ or assimilate to the phonological phenomena in Spanish.

## 2.3. Linguistic phenomena
### 2.3.1. Lexical stress
Stress is the emphasis of a syllable that speakers hear relative to the other syllables in the prosodic word [@hualde2005sounds]. 
The position within words and acoustic correlates change drastically across languages.
In English and Spanish lexical stress has no fixed position because lexical stress is placed differently to contrast meanings. 
The contrastive function is much more often used in Spanish than in English.
In English lexical stress predominantly distinguishes heteronyms or pairs of verbs-nouns that have no segmental differences, such as to “proDUCE,” verb vs “PROduce,” noun. 
In Spanish, lexical stress differentiates all kinds of information, such as verbal tense and person (*CANto* 'I sing' vs _canTÓ_ 's/he sang'), nouns (*PApa* 'potato' vs _paPÁ_ 'dad'), or nouns from verbs (*TÉRmino* 'term' vs _terMIno_ 'I finish' vs _termiNÓ_ 's/he finished').
In English lexical stress is signaled by longer vowel duration and better quality [@cooper2002constraints; @cutler1986forbear] mainly, although other cues such as intensity and pitch (F0) [@beckman1986stress; @fry1955duration; @fry1958experiments; @fry1965dependence; @sluijter1996spectral; @sluijter1997spectral] play minor roles.
In Spanish, pitch (F0) is higher for stressed syllables and lower for unstressed syllables, stressed syllables are louder, and stressed syllables are usually slightly longer [@hualde2005sounds; @ortega2006phonetic; @ortega2007disentangling; @ortega2009perception].
Thus it seems that perception and processing of pitch is more crucial in Spanish than in English.

The acoustic characteristics of lexical stress are also related to the prosody of the language. 
Vowel reduction is linked to the rhythmic pattern of stressed-timed languages, like English, where the intervals between stressed syllables have similar duration.
In this type of languages, the stressed syllable signals a rhythmic unit that can be composed of multiple sub-units until the next stressed syllable and thus rhythmic unit arrives.
Spanish, on the contrary, is defined sometimes a syllable-timed language, as syllable duration is quite stable, and thus vowel trajectory length is approximately the same for all syllables regardless of their tonicity [@colantoni2015second; @hualde2005sounds]. 
In these languages, stress is simply part of a syllable, and each syllable is a new rhythmic unit. 

English and Spanish therefore share the use of lexical stress as contrastive, but the correlates in each language and the consequent rhythmic structure are different.
Consequently, English speakers can only transfer phonological knowledge (i.e., contrastive function) to Spanish.
Since they need to start paying attention to pitch and to a new rhythm, better innate abilities to perceive and process pitch and rhythm in general may help in the L2 Spanish acquisition process.

### 2.3.2. Tone
Tones are the pitch contour patterns of the voiced part in syllables [@chao1968grammar]. 
Only a few languages use tones contrastively at the lexical level.
One such language is Mandarin Chinese, whose dialects (e.g., from Beijing and Tianjin) use all pitch (F0) contour in the syllable as the sole acoustic correlate for tones [@gandour1978perception; @zhu2015tone]. 
Since pitch is the only correlate, syllables in Mandarin Chinese have all similar duration.
Therefore, Mandarin is described as a syllable-timed language in terms of rhythm [@grabe2002durational; @lin2007mandarin; @mok2009syllable], just like Spanish.     

Mandarin Chinese shares with Spanish that they are syllable-time languages.
Mandarin learners of Spanish do not need therefore to learn a new rhythm.
In consequence, their innate rhythic abilities may not be consequential.
In contrast, they need to learn a new way to use pitch (contour in Mandarin vs height in Spanish).
Hypothetically, better pitch perception and processing abilities may thus also help in processing pitch for lexical stress and use it for speech prediction.
Lastly, given the practice L1 Spanish speakers have using lexical stress to predict verb tense, they will not be affected anymore by their innate pitch and rhythm abilities, as they will have overcome any association hurdles.