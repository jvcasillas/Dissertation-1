\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[english,man]{apa6}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={WHAT'S NEXT: THE ROLE OF RHYTHMIC, MELODIC AND VISUAL-SPATIAL ANTICIPATION ABILITIES, L1 TRANSFER AND L2 PROFICIENCY ON L2 ANTICIPATION OF VERB SUFFIXES},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\begin{center}\begin{threeparttable}}
%   {\end{threeparttable}\end{center}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\makeatother
\shorttitle{Contains only auditory-related sections}
\author{Laura Fernandez Arroyo\textsuperscript{Rutgers University}}
\affiliation{\phantom{a}}
\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=english]{babel}
\else
  % load polyglossia as late as possible as it *could* call bidi if RTL lang (e.g. Hebrew or Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\fi

\title{WHAT'S NEXT: THE ROLE OF RHYTHMIC, MELODIC AND VISUAL-SPATIAL ANTICIPATION ABILITIES, L1 TRANSFER AND L2 PROFICIENCY ON L2 ANTICIPATION OF VERB SUFFIXES}

\date{}

\begin{document}
\maketitle

\hypertarget{acoustic-correlates}{%
\subsection{Acoustic correlates}\label{acoustic-correlates}}

\emph{Lexical stress}

Stress is the prominence of a syllable that speakers hear relative to the other syllables in the prosodic word (Hualde (2005)). Lexical stress has no fixed position in either English or Spanish and thus is phonologically contrastive at the lexical level in both languages. That is, lexical stress can be used to distinguish between words. The contrastive use is nevertheless much more typical in Spanish than in English. In English it is used predominantly to distinguish English heteronyms or pairs of verbs-nouns that have no segmental differences. For example, to \enquote{PROduce,} verb vs. \enquote{proDUCE,} noun. In Spanish, lexical stress differentiates all kinds of words and information, such as verbal tense and person (CANto \enquote{I sing} vs canTÓ \enquote{s/he sang}), or nouns (PApa \enquote{potato} vs paPÁ \enquote{dad}), or nouns from verbs (TÉRmino \enquote{term} vs terMIno \enquote{I finish} vs termiNÓ \enquote{s/he finished}).
The major emphasis of the stressed syllable is caused by different acoustic correlates depending on the language. Stress is the combined result of many parameters in action, among which we can find F0 variations, duration, overall intensity, and vowel formant frequencies (Gordon and Roettger (2017)), and the different importances or weights assigned to each of these correlates cause the nature of stressed syllables in each language to vary. In Spanish, the most reliable cues to stress are pitch (F0) height, duration and intensity (Hualde (2005); Ortega-Llebaria (2006); Ortega-Llebaria and Prieto (2007); Ortega-Llebaria and Prieto (2009)). Pitch is high for stressed syllables and low for unstressed syllables; regarding intensity, stressed syllables are louder; and lastly, stressed syllables are usually slightly longer. In contrast, the main cues in English are vowel duration and quality (Cooper, Cutler, and Wales (2002); Cutler (1986)), although other cues such as intensity and pitch (F0) height (Beckman (1986); Fry (1955); Fry (1958); Fry (1965); Sluijter and Van Heuven (1996); Sluijter, Van Heuven, and Pacilly (1997)) play minor roles. Thus, unstressed vowels are reduced to {[}ə{]} and their formants are centralized. The different weight assigned to each cue in these languages may explain why L1 English speakers encounter difficulties in Spanish lexical stress perception (Face (2006); Ortega-Llebaria, Gu, and Fan (2013)) and production (Lord (2007)).
The different cue weights have also effects on the prosodic structure of the language. Vowel reduction causes English to be a stressed-timed language, where the intervals between stressed syllables have similar durations. Spanish, on the contrary, is a syllable-timed language, as all syllables last approximately the same regardless of their tonicity (Colantoni, Steele, Escudero, and Neyra (2015); Hualde (2005)). In English, the stressed syllable signals a rhythmic unit that can be composed of multiple sub-units until the next stressed syllable and thus rhythmic unit arrives; while in Spanish, the stress is simply part of a syllable, and each syllable is a new rhythmic unit.
The different weights assigned to each cue in different languages may also be one of the factors determining the differences in lexical stress processing particular to each language. As mentioned above, lexical stress helps activation of lexical entries in L1 Spanish (Soto-Faraco, Sebastián-Gallés, and Cutler (2001)), such that a prosodically matching cue to the target (prinCI \textgreater{} prinCIpio, \enquote{start}) results in shorter and more accurate decision making times, when compared to mismatching cues (PRINci \textgreater{} prinCIpio). These results are taken to suggest that participants in Soto-Faraco et al. (2001) study were anticipating the lexical element based on suprasegmental cues such as lexical stress. Contrarily, it is unclear whether L1 English speakers are able to use stress alone as a cue to anticipate and facilitate lexical activation. On the one hand, Cooper et al. (2002) tested L1 English speakers in a similar study to that of Soto-Faraco et al. (2001) and found that the English natives were only able to use the suprasegmental cues when more than one syllable of the word was present. On the other hand, Perdomo and Kaan (2019) did find in an eye-tracking study that the presence of lexical stress elicited fixations on the oncoming target noun. It is possible that the L1 English speakers were using low emphasis in previous syllables to activate the cue role for lexical stress; similarly to what the L1 Spanish might have done, as the target words came at the end of a context sentence.
This difference in performance amongst studies is probably due to the weights assigned to each cue and how it is presented. That is, to the larger reliance of English L1 speakers on duration to process Spanish lexical stress transferred from L1 English processing against the reliance of Spanish L1 speakers on other cues, such as pitch and intensity, that are discarded by the English speakers (Ortega-Llebaria et al. (2013)). The differences and similarities in cue weighting can no doubt influence lexical stress perception in an L2. For example, Cooper et al. (2002) found that the similar distribution of stress in Dutch and English helped L1 Dutch learners of English transfer their knowledge of lexical stress to process it properly in L2 English. But German speakers perceive stress significantly more poorly in another free lexically stressed language such as Spanish than L1 speakers (Schwab and Dellwo (2016)); again, maybe due to the different weights assigned to each cue. Following this line, Vickie and Andruski (2010) suggested that language background in the L1 can affect how lexical stress is perceived in an L2, when their L1 Mandarin participants used different cues to process L2 English lexical stress as compared to L1 English participants.
The influence of the different weights assigned to each cue might extend beyond perception and affects anticipation as well. The studies above suggest that the acoustic properties of prosody are essential in processing and activating language. Studies like the one by Vickie and Andruski (2010) further suggest that speakers can resort to their prosodic abilities in the L1 in order to process other prosodic structures in their L2 absent in their L1. That would be the case of L1 speakers of tonal languages learning L2 stressed languages. Extending this hypothesis, L2 speakers might be able to reassemble prosodic structure and its basis in order to promote L2 anticipation so as to reduce the processing cognitive load.

\emph{Tones}
Tones are the pitch contour patterns of the voiced part in syllables (Chao, 1968). Many languages use tones, or changes in pitch-contour, at a phrasal level for pragmatic purposes. However, only a few use tones contrastively at a lexical level. The acoustic correlates for tones vary across languages: some use only pitch (e.g., Mandarin Chinese), whereas others also use length and/or register (e.g., Cantonese Chinese). Relevant to my dissertation with Mandarin Chinese speakers, in most Mandarin Chinese dialects (e.g., from Beijing and Tianjin), the main acoustic correlate for tones is changes in pitch (F0) contour or changes in pitch height within a syllable (Gandour, 1978; Zhu \& Wang, 2015). Importantly, tones in Mandarin Chinese do not cause shorter and longer syllables. Therefore, Mandarin is a syllable-timed language in terms of rhythm (Grabe \& Low, 2002; Lin \& Wang, 2007; Mok, 2009), just like Spanish.
In Mandarin, tones facilitate word recognition (Malins \& Joanisse, 2010). They are nevertheless not the most important factor in that process, as vowel especially but also consonant identity comes first (Hu et al., 2012). In other words, while tones confirm that the correct word is activated, the main vehicle to access a lexicon entry in Mandarin are other cues, mainly segmental cues. Wiener and Turnbull (2016) investigated the degree to which segmental (vocalic and consonantal phonemes) and suprasegmental cues (tones) constrained lexical access in Mandarin Chinese. L1 Mandarin speakers were presented different types of stimuli containing different types of violations (tonotactic, phonactic) and had to decide as fast as possible if what they were hearing was a real word or not. Words with tonotactic violations were more often endorsed as real words than other types of violations, such as vowel or consonant change. These results led the authors to conclude that tone information is not as important as consonant and especially vowel information in lexical access. Hu et al. (2012) reached an akin conclusion in a ERP study. Hu et al. (2012) studied the relevance of tone and vowel information at different stages of lexical access, for which they selected fixed Chinese idioms and isolated words as context. Vowel mismatches evoked earlier (N1 effect) than tone mismatches (N400). The N1 effect was taken to suggest that vowel was playing a role on word selection, while the N400 effect signals a failure of the integration of the word in the sentential context. Sereno and Lee (2015) obtained similar results in two priming experiments in Mandarin. Participants were slower in discriminating words where the only difference was tonal. Primings where both vowel and tone matched where the fastest one, followed by primings where only vowel matched. The three studies led to the conclusion that the functions of vowels and tones in Mandarin are distinct. Namely, that vowel plays a major role in activation, while the role of tone is integration in the higher context.
The knowledge of the nature and function of tones in the L1 can affect L2 tone learning positively by providing a background knowledge to which learners can resort to acquire the L2 tones. However, L1 tone knowledge can also affect L2 tone learning negatively when transferring the L1 tone knowledge interferes with the nature of the L2 tones. Li et al. (2017) examined the influence of the L1 tonal knowledge in the acquisition of L2 tones in children. These children were L1 Cantonese speakers learning L2 Mandarin, and they had issues in categorizing Mandarin tones 1 and 4, as these tones would be assimilated to the same tone 1 category in Cantonese. In the case of these children, being a native speaker to a tonal language helped them in the perception of Mandarin tones 2 and 3, but it hindered perception of other L2 tones because the knowledge transfer from L1 tones disagreed from the L2 tone structure and interfered with it.
Although it seems the role of tones is not as pre-eminent of lexical stress in Spanish, it still helps in word activation and must be encoded in the lexicon. Mandarin speakers need to pay attention to the pitch variations in order to assign the correct tone to the word they are hearing. Since pitch variations are the basis for lexical stress in Spanish, Mandarin speakers might be able to transfer their sensitivity to pitch changes to process and use pitch to anticipate linguistic information more easily than English speakers. Although English also has lexical stress, it discards the acoustic correlate, so learning to distinguish the pitch variations may be more difficult than simply transferring the sensitivity, and thus Mandarin speakers may outperform English speakers in using lexical stress to anticipate verbal tense in L2 Spanish.

\hypertarget{music}{%
\section{Music}\label{music}}

There is abundant evidence supporting a cognitive association between music and language. From a visual standpoint, higher musical abilities yield more effective reading comprehension skills in children (Anvari et al., 2002). Regarding acoustic processing, musical abilities influence positively how linguistic sound is encoded in adults (Wong et al., 2007) as well as in children (Magne et al., 2006), and also in a foreign language (Marques et al., 2007; Slevc \& Miyake, 2006). The positive effect of musical experience on language is also reflected on enhanced cortical ERPs: Even when shared acoustic resources, like pitch, are kept stable across conditions, musicians process faster speech than non-musicians and the brainstem responses happen earlier (Musacchia et al., 2007).
The influence of music in language starts early during childhood. Regarding syntax, Jentschke et al. (2014) found that music and language follow a similar time course of acquisition of syntactic regularities. Dége and Schwarzer (2011) compared the effects of a music program and a phonological skills program on children's phonological awareness, and they found that phonological awareness of small and large phonological units improved from a pre-test to a post-test for both treatment groups, while no improvement was observed for a control group. In a similar fashion, Lebedeva and Kuhl (2010) explored whether infants were able to discriminate phonetic from melodic patterns in songs. Results indicated that 11-month-old infants could discriminate between 4-note sung melodies, but the infants did not show any preference towards familiar or unfamiliar spoken strings containing a syllable order change when the intonation contour remained stable for both strings. However, when the melodies were presented without the phonetic cue, and thus were not sung only instrumental, infants did not show any preference, suggesting that they were not discriminating the melodies based on their melodic contour alone. Taken together, the findings of the tasks Lebedeva and Kuhl (2010) deployed suggest that infants use the phonetic content of melodies to process a melodic line, even if they are not able to understand fully what they are hearing. Findings like the one by Lebedeva and Kuhl (2010) and the resemblance in processing between music and language have even led some scholars to suggest that language is processed as a type of music by the early developing brain (Koelsch and Siebel, 2005).
The music-language association found in children is also reported in adult native speakers and L2 learners. ERPs have shown that music-syntactic and language-syntactic irregularities produce similar reduced responses in amusic individuals, while semantic irregularities were unaffected (Sun et al., 2018). Yu et al. (2017) conducted a fMRI study particularly designed to test how different components of music and first language are associated. They compared the results from a group of participants undergoing music training group and a control group in language tests (an animal-word cancelation test and an onset cancelation test) and music tests (an interval test and a rhythmic test). The training group performed better in all tests, and what is more, the accuracy in the animal-word cancelation test and the accuracy in the interval test were positively correlated.
With regard to L2 learners, Ghaffarvand Mokari and Werner (2018) studied whether there was any kind of association between general musical abilities and L2 vowel learning with L1 Azerbaijani learners of English. The participants were split into control group and experimental group, who underwent phonetic training. Both groups completed music ability tests (chord analysis, pitch change and tonal memory) and linguistic tests (discrimination and production). Results point that the increased accuracy in discrimination and production of L2 sounds from the pre- to the post-test is not related to the musical ability in general, although tonal memory was significantly correlated to the gained skill to discriminate L2 vowels. Regarding rhythm, Cason et al. (2019) investigated how rhythmic abilities were related to the perception and production of L2 stress placement. Rhythmic production scores were a reliable predictor of L2 stress placement. Swaminathan et al. (2018) explored if and how rhythm perception was associated with reading abilities in both the L1 and L2, and they found however that better rhythm perception was not associated with increased reading abilities in either type of language.
In sum, research suggests that at least some cognitive faculties underlying music and language are shared. Sharing the faculty might be determined by the area we are focusing on, like speech vs reading. Previous research also suggests that anticipation is an important mechanism in both realms, especially in relation to rhythm, but to the best of my knowledge, no study so far has tested whether this processing mechanism is also interdependent to each domain or shared. Positive correlation between linguistic anticipation and other types of anticipation common in music, such as rhythmic or tonal, could indicate that there are domain-general predictive cognitive mechanisms underpinning auditory anticipation as a whole.

\hypertarget{pitch}{%
\subsection{Pitch}\label{pitch}}

Pitch is the frequency associated to a sound wave; this frequency places the sound within a scale ranging from low to high in perception (Klapuri, 2006). Pitch in music plays a similar role to that it plays in language. In language, a listener must process the melodic contour of speech, that is, the changes up and down of pitch in order to understand the meaning conveyed by prosody (Dilley, 2005). For example, a rising pitch contour signals a question (\enquote{Coffee?}), while a falling intonation signals a statement (\enquote{Coffee.}). Apart from the phrasal and sentential intonation, pitch also affects tones and predominance against other syllables within a word. In music, pitch affects the note we hear. Both domains are related, and extensive research with atypical populations like amusics or individuals with Williams Syndrome (WS) have been one of the main foci proving that relationship. Research with typical population has also provided evidence that pitch processing in music is related to linguistic abilities, but not always.
Starting with typical populations, L1 speakers of tonal languages are better and faster at discriminating and detecting absolute pitch in music (e.g., Chua \& Brunt, 2014; Deutsch et al., 2009; Tsukada et al., 2015), since tones in music depend on pitch, like tones in many languages. Although music in different cultures around the world vary on how their scales are organized (as well as on the patterns of strong-weak pulses; Morrison \& Demorest, 2009), the basis for the scales is always pitch. Pitch collections may be different from one music culture to another, but the range is shared. Bidelman et al. (2013) tested how L1 Cantonese listeners, non-tonal speaking musically-trained participants and a non-tonal speaking non-musician control group performed in a three-alternative forced-choice task, an auditory inspection time paradigm, a short-term pitch memory task and a melody discrimination task to compare their measures of auditory pitch acuity, music perception, and general cognitive ability. The Cantonese speakers outperformed the control group on most pitch and music perception measures, and performed similarly to the musicians' group. Because of the good results for the Cantonese group, who had no musical knowledge, the authors suggested that music and language abilities transfer bidirectionally. Additionally, musicianship contributes to better lexical tone perception even in a tonal language (Tang et al., 2016).
In contrast to the studies described so far, other studies have failed to obtain a music-language association. Chan and Leung (2019) studied the creation of L2 tone-segment connections in musicians and non-musicians. Their results indicate that musical experience did not promote tone-segment connection in a tonal L2 when the L1 was non-tonal, but a tonal L1 did promote L2 tone-segment connection and the creation of a rule-like system of L2 tone information. Likewise, only musical experience can be beneficial in relative pitch perception in music, while tonal-language experience has no effect in relative pitch perception performance (Ngo et al., 2016). From this set of results, it is possible to deduce that there is a limit in the reciprocal influence between music and language (musical and linguistic pitch for the purpose of this project), but where this limit lies has not been ascertained.
,In a study in amusia (defined as reduced or lost musical ability, such that comprehension and production of music, and ability to read and/or write musical notation are impeded, Pearce, 2005), Patel et al. (2008) observed that amusic individuals distinguish changes in pitch but are unable to detect the direction of the change, such that they can discriminate changes in intonation, but cannot really tell what the prosodic change means. This ability to perceive pitch and its changes is probably one of the most studied aspects of the relationship between music and language. Further evidence of shared cognitive mechanisms between music and language is provided by research on atypical populations. Martínez-Castilla and Sotillo (2014) investigated whether children with WS processed pitch in music and language through common mechanisms. For that purpose, the WS children and typically developing children completed a musical pitch discrimination task, a short-item discrimination task and a long-item discrimination task. The WS group performed well above chance in all tasks, but their scores were significantly lower than those of the control group of typically developing children. In the case of the experimental group, the scores for the musical pitch discrimination-task and the short-item discrimination task were also correlated.
In sum, some areas of music and language are connected, but influence from music to language and vice versa may be blocked. What are these specific areas is not completely known. While the shared processing mechanisms between music and language have been previously researched, no study so far has looked into how the anticipation mechanisms may be shared by the different auditory domains, or how the different acoustic correlates may affect anticipation in these domains. Separately, there is evidence for the existence of prediction processes in music (e.g., Salimpoor et al., 2015) as well as in language. Like in language, anticipation in music can be cued through syntactic structure (Sammler et al., 2013) or acoustic properties (Loui \& Wessel, 2007). Anticipation has been particularly examined in rhythm, as synchronization to a rhythm is intrinsically keeping track of the interval structure, and therefore anticipation of the next event.

\hypertarget{rhythm}{%
\subsection{Rhythm}\label{rhythm}}

Rhythm is defined as a pattern of recurrent time intervals that usually occur periodically (Berlyne, 1971). This period nature allows to predict the start of an interval or the next recurring event based on what has already been perceived (Fraisse, 2013; Martin, 1972). Rhythms can be perceived visually, as research with deaf individuals show. Iversen et al. (2015) investigated how the accuracy of visual timing abilities and rhythmic perception are affected by developmental experience by comparing deaf and hearing adults in a tap synchronization task with three types of isochronous rhythmic stimuli: a flashing visual square, an animated bouncing ball and auditory tones. Results revealed that synchronization driven by a silent moving visual stimulus can be as accurate as that driven by sound, and that deaf individuals could synchronize their movements as well or even better than hearing individuals when presented with visual stimuli. However, rhythms are perceived primarily through our auditory pathways, like speech. Rhythms perceived auditorily are the ones that have been used to research hearing populations, regardless of whether the studies also included visual tasks, like reading. Swaminathan et al. (2018) studied the existence of an association between rhythm perception and reading ability in adults in their L1 and L2 and found no evidence. However, Cason \& Schön (2012) did find that on-beat sequence primes yielded faster L1 speech processing than off-beat primes in adults through EEG recordings. The contradictory results obtained from these populations suggest that the effects of rhythm in hearing adults may not be as extensive as in deaf individuals in the visual domain. Maybe because hearing individuals do not need to hone their visual perception and processing abilities. Alternatively, there might be some \enquote{loss} in the transfer of rhythmic abilities from a visual domain to an auditory domain or vice versa, causing the relationship between rhythm and language in different modalities to be weaker. This conclusion would be supported by findings suggesting that within a modality, the relationship between rhythm and language, in this case speech, is stronger.
Like in tone and pitch, there is evidence that rhythm is associated with language in children, L2 speakers, and atypical populations (beat-deafness, dyslexia and WS) within the auditory modality. Starting with children, Carr et al. (2014) had their typically developing children complete a series of linguistic, verbal memory, rhythmic and musical in general tasks to investigate the relationship between rhythmic synchronization and the encoding of syllable envelops in pre-schoolers. Those children who synchronized better also had better perceptual and cognitive language skills. They were faster at naming objects and color, and their neural encoding of the speech envelope was also better. Encoding of sound is also favored by rhythmic perception in the L2 experience. Cason et al. (2019) and Bhatara et al. (2015) tested stress placement and rhythmic abilities in L1 French speakers and L2 Spanish. The rhythmic production scores of the participants predicted correct placement of nuclear stress in a L2 lexical stress imitation task; and rhythm perception was actually positively associated with L2 learning experience. These studies suggest that rhythm is indeed related to speech in typical populations within a modality, and specifically, within the auditory domain.
Studies with atypical populations have also yielded positive results supporting a relationship between heard rhythms and speech. Lagrois et al. (2019) tested beat-deaf individuals, that is, individuals who have a documented deficit in tracking musical beat, to analyze how they synchronized to speech. Participants had to tap along sentences in three conditions: regularly spoken, naturally spoken and sung, and were compared to a matched control group. The beat-deaf group showed more variability in its tapping regardless of the condition. The irregularity remained when they tapped to a metronome or at their own pace and was larger than in the typical non-musician control group. These results show that the deficiency in time-keeping mechanisms might underlie both domains rather than being domain-specific. In the same vein, Persici et al. (2019) supported this conclusion when they compared the anticipation abilities of children with developmental dyslexia and typically developing children, and the results showed that children with dyslexia achieved lower scores than their peers in morphosyntactic and rhythmic processing. Therefore, noun prediction based on this information looked alike for both groups, but phonologically- and/or grammatically-based anticipation was hindered for the dyslexic children. Similarly, Sun et al. (2018) explored brain responses to syntactic violations in music and language in amusics. Brain responses to incongruities in both domains were reduced, but brain responses at later stages of processing, that is, during semantic processing, were unaffected or showed the same pattern as brain responses in typical individuals to semantic incongruencies. Based thus on previous findings suggesting the delayed anticipation in rhythm, phonology and morphosyntax, it is not unlikely that structure-based prediction is contingent to a cognitive mechanism common to the auditory mode in general, rather to domain-specific abilities.
Movement is closely tied to music. Moving one or some parts of the body is a natural reaction to a rhythm. As early as 1949, Fraisse et al.~found that humans rock to a musical rhythm as young as one year of age, and we are able to tap along metronome beats by the age of three or four. This sensorimotor behavior is usually based on anticipation of when the next beat is coming, especially at slow tempi (Nozaradan et al., 2016; Van der Steen et al., 2015), and the anticipation grows in accuracy with years of musical training (Nozaradan et al., 2016). Prediction of oncoming beats is also present in rhythms with tempo changes (van der Steen et al., 2015). The ability to synchronize to beats or changes in rhythm is conditioned by (1) motor limits, (2) the fractured memory of a slow sequence, (3) sensory difficulties at perceiving successiveness at fast tempi (Bartlett and Bartlett, 1959), and (4) the complexity of the rhythm (Fraisse \& Repp, 2012). However, the adaptation is fast, since it can take as few as three consecutive taps accompanying each a consecutive beat in a novel rhythm to achieve a relative simultaneity (Fraisse \& Repp, 2012). The rhythm structure affects the synchronization pattern, as some tempo speeds are easier to adjust to than others. When the interval between beats is on or under 1500 ms, the synchronization behavior tends towards anticipation of the next beat, while this anticipation rate starts to decrease with beat onset intervals of 1800 ms (Miyake et al., 2004). From the previous investigations on rhythm, we can see that synchronization is basically prediction of the rhythm timings.

The findings on the relationship between rhythmic abilities and linguistic competence suggest that this relationship might only surface in certain cases, and that transfer from modalities may affect how strong the relationship is. Some of the factors conditioning the existence of a connection between the two domains are atypical development, early stages of cognitive development, nature of L1, or linguistic area in L2 learning. Investigations from other auditory areas, such as speech anticipation, may clarify if and how rhythmic abilities and linguistic abilities are related. Other areas that might be helpful in establishing the nature of the rhythm-language relationship is taking into account how rhythm imbibes from visual and spatial cues. One of the main visual-spatial cues related to rhythm is movement. Movement, like language, is entrenched in our everyday life. We use movement to respond to virtually everything. When driving, we anticipate what other drivers or pedestrians are going to do and adjust accordingly by making foot, manual and head movements. When catching an object in a free fall, we anticipate (more or less successfully) the trajectory and the speed to move our arm and hand fast enough and try to grab the object before it hits the floor.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study.

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

\hypertarget{material}{%
\subsection{Material}\label{material}}

\hypertarget{procedure}{%
\subsection{Procedure}\label{procedure}}

\hypertarget{data-analysis}{%
\subsection{Data analysis}\label{data-analysis}}

We used R (Version 3.6.1; R Core Team, 2019) for all our analyses.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-beckman1986stress}{}%
Beckman, M. E. (1986). \emph{Stress and non-stress accent} (Vol. 7). Walter de Gruyter.

\leavevmode\hypertarget{ref-colantoni2015second}{}%
Colantoni, L., Steele, J., Escudero, P., \& Neyra, P. R. E. (2015). \emph{Second language speech}. Cambridge University Press.

\leavevmode\hypertarget{ref-cooper2002constraints}{}%
Cooper, N., Cutler, A., \& Wales, R. (2002). Constraints of lexical stress on lexical access in english: Evidence from native and non-native listeners. \emph{Language and Speech}, \emph{45}(3), 207--228.

\leavevmode\hypertarget{ref-cutler1986forbear}{}%
Cutler, A. (1986). Forbear is a homophone: Lexical prosody does not constrain lexical access. \emph{Language and Speech}, \emph{29}(3), 201--220.

\leavevmode\hypertarget{ref-face2006cognitive}{}%
Face, T. L. (2006). Cognitive factors in the perception of spanish stress placement: Implications for a model of speech perception. \emph{Linguistics}, \emph{44}(6), 1237--1267.

\leavevmode\hypertarget{ref-fry1955duration}{}%
Fry, D. B. (1955). Duration and intensity as physical correlates of linguistic stress. \emph{The Journal of the Acoustical Society of America}, \emph{27}(4), 765--768.

\leavevmode\hypertarget{ref-fry1958experiments}{}%
Fry, D. B. (1958). Experiments in the perception of stress. \emph{Language and Speech}, \emph{1}(2), 126--152.

\leavevmode\hypertarget{ref-fry1965dependence}{}%
Fry, D. B. (1965). The dependence of stress judgments on vowel formant structure. In \emph{Phonetic sciences} (pp. 306--311). Karger Publishers.

\leavevmode\hypertarget{ref-gordon2017acoustic}{}%
Gordon, M., \& Roettger, T. (2017). Acoustic correlates of word stress: A cross-linguistic survey. \emph{Linguistics Vanguard}, \emph{3}(1).

\leavevmode\hypertarget{ref-hualde2005sounds}{}%
Hualde, J. I. (2005). \emph{The sounds of spanish with audio cd}. Cambridge University Press.

\leavevmode\hypertarget{ref-lord2007role}{}%
Lord, G. (2007). The role of the lexicon in learning second language stress patterns. \emph{Applied Language Learning}, \emph{17}(1/2), 1.

\leavevmode\hypertarget{ref-ortega2006phonetic}{}%
Ortega-Llebaria, M. (2006). Phonetic cues to stress and accent in spanish. In \emph{Selected proceedings of the 2nd conference on laboratory approaches to spanish phonetics and phonology} (pp. 104--118). Cascadilla Proceedings Project Somerville, MA.

\leavevmode\hypertarget{ref-ortega2013english}{}%
Ortega-Llebaria, M., Gu, H., \& Fan, J. (2013). English speakers' perception of spanish lexical stress: Context-driven l2 stress perception. \emph{Journal of Phonetics}, \emph{41}(3-4), 186--197.

\leavevmode\hypertarget{ref-ortega2007disentangling}{}%
Ortega-Llebaria, M., \& Prieto, P. (2007). Disentangling stress from accent in spanish: Production patterns of the stress contrast in deaccented syllables. \emph{Amsterdam Studies in the Theory and History of Linguistic Science Series 4}, \emph{282}, 155.

\leavevmode\hypertarget{ref-ortega2009perception}{}%
Ortega-Llebaria, M., \& Prieto, P. (2009). Perception of word stress in castilian spanish. \emph{Phonetics and Phonology: Interactions and Interrelations}, \emph{306}, 35.

\leavevmode\hypertarget{ref-perdomo2019prosodic}{}%
Perdomo, M., \& Kaan, E. (2019). Prosodic cues in second-language speech processing: A visual world eye-tracking study. \emph{Second Language Research}, 0267658319879196.

\leavevmode\hypertarget{ref-R-base}{}%
R Core Team. (2019). \emph{R: A language and environment for statistical computing}. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from \url{https://www.R-project.org/}

\leavevmode\hypertarget{ref-schwab2016use}{}%
Schwab, S., \& Dellwo, V. (2016). The use of the odd-one-out task in the study of the perception of lexical stress in spanish by german-speaking listeners?

\leavevmode\hypertarget{ref-sluijter1996spectral}{}%
Sluijter, A. M., \& Van Heuven, V. J. (1996). Spectral balance as an acoustic correlate of linguistic stress. \emph{The Journal of the Acoustical Society of America}, \emph{100}(4), 2471--2485.

\leavevmode\hypertarget{ref-sluijter1997spectral}{}%
Sluijter, A. M., Van Heuven, V. J., \& Pacilly, J. J. (1997). Spectral balance as a cue in the perception of linguistic stress. \emph{The Journal of the Acoustical Society of America}, \emph{101}(1), 503--513.

\leavevmode\hypertarget{ref-soto2001segmental}{}%
Soto-Faraco, S., Sebastián-Gallés, N., \& Cutler, A. (2001). Segmental and suprasegmental cues for lexical access in spanish. \emph{Journal of Memory and Language}, \emph{45}, 412--432.

\endgroup

\end{document}
