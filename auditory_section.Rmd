---
title             : "WHAT’S NEXT: THE ROLE OF RHYTHMIC, MELODIC AND VISUAL-SPATIAL ANTICIPATION ABILITIES, L1 TRANSFER AND L2 PROFICIENCY ON L2 ANTICIPATION OF VERB SUFFIXES"
shorttitle        : "LINGUISTIC AND NON-LINGUISTIC ANTICIPATION IN L1 AND L2 SPEAKERS"

author: 
  - name          : "Laura Fernandez Arroyo"
    # affiliation   : ""
  #   corresponding : yes    # Define only one corresponding author
  #   address       : "Postal address"
  #   email         : "my@email.com"
  # - name          : "Ernst-August Doelle"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Rutgers University"
#   - id            : "2"
#     institution   : "Konstanz Business School"
# 
# authornote: |
#   Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
# 
#   Enter author note here.
# 
# abstract: |
#   One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
#   
#   Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
#   
#   One sentence clearly stating the **general problem** being addressed by  this particular study.
#   
#   One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
#   
#   Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
#   
#   One or two sentences to put the results into a more **general context**.
#   
#   Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
#   
#   <!-- https://tinyurl.com/ybremelq -->
#   
# keywords          : "keywords"
# wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences, include = FALSE, echo = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```


## Acoustic correlates
_Lexical stress_

Stress is the prominence of a syllable that speakers hear relative to the other syllables in the prosodic word [@hualde2005sounds]. Lexical stress has no fixed position in either English or Spanish and thus is phonologically contrastive at the lexical level in both languages. That is, lexical stress can be used to distinguish between words. The contrastive use is nevertheless much more typical in Spanish than in English. In English it is used predominantly to distinguish English heteronyms or pairs of verbs-nouns that have no segmental differences. For example, to “PROduce,” verb vs. “proDUCE,” noun. In Spanish, lexical stress differentiates all kinds of words and information, such as verbal tense and person (CANto “I sing” vs canTÓ “s/he sang”), or nouns (PApa “potato” vs paPÁ “dad”), or nouns from verbs (TÉRmino “term” vs terMIno “I finish” vs termiNÓ “s/he finished”).
The major emphasis of the stressed syllable is caused by different acoustic correlates depending on the language. Stress is the combined result of many parameters in action, among which we can find F0 variations, duration, overall intensity, and vowel formant frequencies [@gordon2017acoustic], and the different importances or weights assigned to each of these correlates cause the nature of stressed syllables in each language to vary. In Spanish, the most reliable cues to stress are pitch (F0) height, duration and intensity [@hualde2005sounds; @ortega2006phonetic; @ortega2007disentangling; @ortega2009perception]. Pitch is high for stressed syllables and low for unstressed syllables; regarding intensity, stressed syllables are louder; and lastly, stressed syllables are usually slightly longer. In contrast, the main cues in English are vowel duration and quality [@cooper2002constraints; @cutler1986forbear], although other cues such as intensity and pitch (F0] height [@beckman1986stress; @fry1955duration; @fry1958experiments; @fry1965dependence; @sluijter1996spectral; @sluijter1997spectral] play minor roles. Thus, unstressed vowels are reduced to [ə] and their formants are centralized. The different weight assigned to each cue in these languages may explain why L1 English speakers encounter difficulties in Spanish lexical stress perception [@face2006cognitive; @ortega2013english] and production [@lord2007role].
The different cue weights have also effects on the prosodic structure of the language. Vowel reduction causes English to be a stressed-timed language, where the intervals between stressed syllables have similar durations. Spanish, on the contrary, is a syllable-timed language, as all syllables last approximately the same regardless of their tonicity [@colantoni2015second; @hualde2005sounds]. In English, the stressed syllable signals a rhythmic unit that can be composed of multiple sub-units until the next stressed syllable and thus rhythmic unit arrives; while in Spanish, the stress is simply part of a syllable, and each syllable is a new rhythmic unit.
The different weights assigned to each cue in different languages may also be one of the factors determining the differences in lexical stress processing particular to each language. As mentioned above, lexical stress helps activation of lexical entries in L1 Spanish [@soto2001segmental], such that a prosodically matching cue to the target (prinCI > prinCIpio, “start”) results in shorter and more accurate decision making times, when compared to mismatching cues (PRINci > prinCIpio). These results are taken to suggest that participants in @soto2001segmental study were anticipating the lexical element based on suprasegmental cues such as lexical stress. Contrarily, it is unclear whether L1 English speakers are able to use stress alone as a cue to anticipate and facilitate lexical activation. On the one hand, @cooper2002constraints tested L1 English speakers in a similar study to that of @soto2001segmental and found that the English natives were only able to use the suprasegmental cues when more than one syllable of the word was present. On the other hand, @perdomo2019prosodic did find in an eye-tracking study that the presence of lexical stress elicited fixations on the oncoming target noun. It is possible that the L1 English speakers were using low emphasis in previous syllables to activate the cue role for lexical stress; similarly to what the L1 Spanish might have done, as the target words came at the end of a context sentence.
This difference in performance amongst studies is probably due to the weights assigned to each cue and how it is presented. That is, to the larger reliance of English L1 speakers on duration to process Spanish lexical stress transferred from L1 English processing against the reliance of Spanish L1 speakers on other cues, such as pitch and intensity, that are discarded by the English speakers (@ortega2013english). The differences and similarities in cue weighting can no doubt influence lexical stress perception in an L2. For example, @cooper2002constraints found that the similar distribution of stress in Dutch and English helped L1 Dutch learners of English transfer their knowledge of lexical stress to process it properly in L2 English. But German speakers perceive stress significantly more poorly in another free lexically stressed language such as Spanish than L1 speakers [@schwab2016use]; again, maybe due to the different weights assigned to each cue. Following this line, @vickie2010cross suggested that language background in the L1 can affect how lexical stress is perceived in an L2, when their L1 Mandarin participants used different cues to process L2 English lexical stress as compared to L1 English participants.
The influence of the different weights assigned to each cue might extend beyond perception and affects anticipation as well. The studies above suggest that the acoustic properties of prosody are essential in processing and activating language. Studies like the one by @vickie2010cross further suggest that speakers can resort to their prosodic abilities in the L1 in order to process other prosodic structures in their L2 absent in their L1. That would be the case of L1 speakers of tonal languages learning L2 stressed languages. Extending this hypothesis, L2 speakers might be able to reassemble prosodic structure and its basis in order to promote L2 anticipation so as to reduce the processing cognitive load.

_Tones_

Tones are the pitch contour patterns of the voiced part in syllables [@chao1968grammar]. Many languages use tones, or changes in pitch-contour, at a phrasal level for pragmatic purposes. However, only a few use tones contrastively at a lexical level. The acoustic correlates for tones vary across languages: some use only pitch (e.g., Mandarin Chinese), whereas others also use length and/or register (e.g., Cantonese Chinese). Relevant to my dissertation with Mandarin Chinese speakers, in most Mandarin Chinese dialects (e.g., from Beijing and Tianjin), the main acoustic correlate for tones is changes in pitch (F0) contour or changes in pitch height within a syllable [@gandour1978perception; @zhu2015tone]. Importantly, tones in Mandarin Chinese do not cause shorter and longer syllables. Therefore, Mandarin is a syllable-timed language in terms of rhythm [@grabe2002durational; @lin2007mandarin; @mok2009syllable], just like Spanish.
In Mandarin, tones facilitate word recognition (@malins2010roles). They are nevertheless not the most important factor in that process, as vowel especially but also consonant identity comes first [@hu2012dissociation]. In other words, while tones confirm that the correct word is activated, the main vehicle to access a lexicon entry in Mandarin are other cues, mainly segmental cues. @wiener2016constraints investigated the degree to which segmental (vocalic and consonantal phonemes) and suprasegmental cues (tones) constrained lexical access in Mandarin Chinese. L1 Mandarin speakers were presented different types of stimuli containing different types of violations (tonotactic, phonactic) and had to decide as fast as possible if what they were hearing was a real word or not. Words with tonotactic violations were more often endorsed as real words than other types of violations, such as vowel or consonant change. These results led the authors to conclude that tone information is not as important as consonant and especially vowel information in lexical access. @hu2012dissociation reached an akin conclusion in a ERP study. @hu2012dissociation studied the relevance of tone and vowel information at different stages of lexical access, for which they selected fixed Chinese idioms and isolated words as context. Vowel mismatches evoked earlier (N1 effect) than tone mismatches (N400). The N1 effect was taken to suggest that vowel was playing a role on word selection, while the N400 effect signals a failure of the integration of the word in the sentential context. @sereno2015contribution obtained similar results in two priming experiments in Mandarin. Participants were slower in discriminating words where the only difference was tonal. Primings where both vowel and tone matched where the fastest one, followed by primings where only vowel matched. The three studies led to the conclusion that the functions of vowels and tones in Mandarin are distinct. Namely, that vowel plays a major role in activation, while the role of tone is integration in the higher context.
The knowledge of the nature and function of tones in the L1 can affect L2 tone learning positively by providing a background knowledge to which learners can resort to acquire the L2 tones. However, L1 tone knowledge can also affect L2 tone learning negatively when transferring the L1 tone knowledge interferes with the nature of the L2 tones. @li2017effects examined the influence of the L1 tonal knowledge in the acquisition of L2 tones in children. These children were L1 Cantonese speakers learning L2 Mandarin, and they had issues in categorizing Mandarin tones 1 and 4, as these tones would be assimilated to the same tone 1 category in Cantonese. In the case of these children, being a native speaker to a tonal language helped them in the perception of Mandarin tones 2 and 3, but it hindered perception of other L2 tones because the knowledge transfer from L1 tones disagreed from the L2 tone structure and interfered with it.
Although it seems the role of tones is not as pre-eminent of lexical stress in Spanish, it still helps in word activation and must be encoded in the lexicon. Mandarin speakers need to pay attention to the pitch variations in order to assign the correct tone to the word they are hearing. Since pitch variations are the basis for lexical stress in Spanish, Mandarin speakers might be able to transfer their sensitivity to pitch changes to process and use pitch to anticipate linguistic information more easily than English speakers. Although English also has lexical stress, it discards the acoustic correlate, so learning to distinguish the pitch variations may be more difficult than simply transferring the sensitivity, and thus Mandarin speakers may outperform English speakers in using lexical stress to anticipate verbal tense in L2 Spanish.

# Music
There is abundant evidence supporting a cognitive association between music and language. From a visual standpoint, higher musical abilities yield more effective reading comprehension skills in children [@anvari2002relations]. Regarding acoustic processing, musical abilities influence positively how linguistic sound is encoded in adults [@wong2007musical] as well as in children [@magne2006musician], and also in a foreign language [@marques2007musicians; @slevc2006individual]. The positive effect of musical experience on language is also reflected on enhanced cortical ERPs: Even when shared acoustic resources, like pitch, are kept stable across conditions, musicians process faster speech than non-musicians and the brainstem responses happen earlier [@musacchia2007musicians].
The influence of music in language starts early during childhood. Regarding syntax, @jentschke2014neural found that music and language follow a similar time course of acquisition of syntactic regularities. @dege2011effect compared the effects of a music program and a phonological skills program on children’s phonological awareness, and they found that phonological awareness of small and large phonological units improved from a pre-test to a post-test for both treatment groups, while no improvement was observed for a control group. In a similar fashion, @lebedeva2010sing explored whether infants were able to discriminate phonetic from melodic patterns in songs. Results indicated that 11-month-old infants could discriminate between 4-note sung melodies, but the infants did not show any preference towards familiar or unfamiliar spoken strings containing a syllable order change when the intonation contour remained stable for both strings. However, when the melodies were presented without the phonetic cue, and thus were not sung only instrumental, infants did not show any preference, suggesting that they were not discriminating the melodies based on their melodic contour alone. Taken together, the findings of the tasks @lebedeva2010sing deployed suggest that infants use the phonetic content of melodies to process a melodic line, even if they are not able to understand fully what they are hearing. Findings like the one by @lebedeva2010sing and the resemblance in processing between music and language have even led some scholars to suggest that language is processed as a type of music by the early developing brain [@koelsch2005towards]. 
	The music-language association found in children is also reported in adult native speakers and L2 learners. ERPs have shown that music-syntactic and language-syntactic irregularities produce similar reduced responses in amusic individuals, while semantic irregularities were unaffected [@sun2018syntactic]. @yu2017shared conducted a fMRI study particularly designed to test how different components of music and first language are associated. They compared the results from a group of participants undergoing music training group and a control group in language tests (an animal-word cancelation test and an onset cancelation test) and music tests (an interval test and a rhythmic test). The training group performed better in all tests, and what is more, the accuracy in the animal-word cancelation test and the accuracy in the interval test were positively correlated.
With regard to L2 learners, @mokari2018perceptual studied whether there was any kind of association between general musical abilities and L2 vowel learning with L1 Azerbaijani learners of English. The participants were split into control group and experimental group, who underwent phonetic training. Both groups completed music ability tests (chord analysis, pitch change and tonal memory) and linguistic tests (discrimination and production). Results point that the increased accuracy in discrimination and production of L2 sounds from the pre- to the post-test is not related to the musical ability in general, although tonal memory was significantly correlated to the gained skill to discriminate L2 vowels. Regarding rhythm, @cason2019rhythmic investigated how rhythmic abilities were related to the perception and production of L2 stress placement. Rhythmic production scores were a reliable predictor of L2 stress placement. @swaminathan2018explaining explored if and how rhythm perception was associated with reading abilities in both the L1 and L2, and they found however that better rhythm perception was not associated with increased reading abilities in either type of language.
	In sum, research suggests that at least some cognitive faculties underlying music and language are shared. Sharing the faculty might be determined by the area we are focusing on, like speech vs reading. Previous research also suggests that anticipation is an important mechanism in both realms, especially in relation to rhythm, but to the best of my knowledge, no study so far has tested whether this processing mechanism is also interdependent to each domain or shared. Positive correlation between linguistic anticipation and other types of anticipation common in music, such as rhythmic or tonal, could indicate that there are domain-general predictive cognitive mechanisms underpinning auditory anticipation as a whole.

## Pitch
Pitch is the frequency associated to a sound wave; this frequency places the sound within a scale ranging from low to high in perception [@klapuri2006introduction]. Pitch in music plays a similar role to that it plays in language. In language, a listener must process the melodic contour of speech, that is, the changes up and down of pitch in order to understand the meaning conveyed by prosody [@dilley2005phonetics]. For example, a rising pitch contour signals a question (“Coffee?”), while a falling intonation signals a statement (“Coffee.”). Apart from the phrasal and sentential intonation, pitch also affects tones and predominance against other syllables within a word. In music, pitch affects the note we hear. Both domains are related, and extensive research with atypical populations like amusics or individuals with Williams Syndrome (WS) have been one of the main foci proving that relationship. Research with typical population has also provided evidence that pitch processing in music is related to linguistic abilities, but not always.
Starting with typical populations, L1 speakers of tonal languages are better and faster at discriminating and detecting absolute pitch in music (e.g., @chua2014effect; @deutsch2009absolute; @tsukada2015perception), since tones in music depend on pitch, like tones in many languages. Although music in different cultures around the world vary on how their scales are organized (as well as on the patterns of strong-weak pulses, @morrison2009cultural), the basis for the scales is always pitch. Pitch collections may be different from one music culture to another, but the range is shared. @bidelman2013tone tested how L1 Cantonese listeners, non-tonal speaking musically-trained participants and a non-tonal speaking non-musician control group performed in a three-alternative forced-choice task, an auditory inspection time paradigm, a short-term pitch memory task and a melody discrimination task to compare their measures of auditory pitch acuity, music perception, and general cognitive ability. The Cantonese speakers outperformed the control group on most pitch and music perception measures, and performed similarly to the musicians’ group. Because of the good results for the Cantonese group, who had no musical knowledge, the authors suggested that music and language abilities transfer bidirectionally. Additionally, musicianship contributes to better lexical tone perception even in a tonal language [@tang2016musical].
In contrast to the studies described so far, other studies have failed to obtain a music-language association in typical associations. @chan2019lexical studied the creation of L2 tone-segment connections in musicians and non-musicians. Their results indicate that musical experience did not promote tone-segment connection in a tonal L2 when the L1 was non-tonal, but a tonal L1 did promote L2 tone-segment connection and the creation of a rule-like system of L2 tone information. Likewise, only musical experience can be beneficial in relative pitch perception in music, while tonal-language experience has no effect in relative pitch perception performance [@ngo2016effects]. From this set of results, it is possible to deduce that there is a limit in the reciprocal influence between music and language (musical and linguistic pitch for the purpose of this project), but where this limit lies has not been ascertained. An area the relationship between pitch and language is clearer is atypical populations.
Research on atypical populations like amusics or individuals with WS has produced larger evidence of a connection between pitch and language.In a study in amusia (defined as reduced or lost musical ability, such that comprehension and production of music, and ability to read and/or write musical notation are impeded, @pearce2005selected), @patel2008speech observed that amusic individuals distinguish changes in pitch but are unable to detect the direction of the change, such that they can discriminate changes in intonation, but cannot really tell what the prosodic change means. This ability to perceive pitch and its changes is probably one of the most studied aspects of the relationship between music and language. Further evidence of shared cognitive mechanisms between music and language is provided by research on atypical populations. @martinez2014pitch investigated whether children with WS processed pitch in music and language through common mechanisms. For that purpose, the WS children and typically developing children completed a musical pitch discrimination task, a short-item discrimination task and a long-item discrimination task. The WS group performed well above chance in all tasks, but their scores were significantly lower than those of the control group of typically developing children. In the case of the experimental group, the scores for the musical pitch discrimination-task and the short-item discrimination task were also correlated.
In sum, some areas of music and language are connected, but influence from music to language and vice versa may be blocked. What are these specific areas is not completely known. While the shared processing mechanisms between music and language have been previously researched, no study so far has looked into how the anticipation mechanisms may be shared by the different auditory domains, or how the different acoustic correlates may affect anticipation in these domains. Separately, there is evidence for the existence of prediction processes in music (e.g., @salimpoor2015predictions) as well as in language. Like in language, anticipation in music can be cued through syntactic structure [@sammler2013syntax] or acoustic properties [@loui2007harmonic]. Anticipation has been particularly examined in rhythm, as synchronization to a rhythm is intrinsically keeping track of the interval structure, and therefore anticipation of the next event.

## Rhythm
Rhythm is defined as a pattern of recurrent time intervals that usually occur periodically [@berlyne1971aesthetics]. This period nature allows to predict the start of an interval or the next recurring event based on what has already been perceived [@fraisse2013rhythm; @martin1972rhythmic]. Rhythms can be perceived visually, as research with deaf individuals show. @iversen2015synchronization  investigated how the accuracy of visual timing abilities and rhythmic perception are affected by developmental experience by comparing deaf and hearing adults in a tap synchronization task with three types of isochronous rhythmic stimuli: a flashing visual square, an animated bouncing ball and auditory tones. Results revealed that synchronization driven by a silent moving visual stimulus can be as accurate as that driven by sound, and that deaf individuals could synchronize their movements as well or even better than hearing individuals when presented with visual stimuli. However, rhythms are perceived primarily through our auditory pathways, like speech. Rhythms perceived auditorily are the ones that have been used to research hearing populations, regardless of whether the studies also included visual tasks, like reading. @swaminathan2018explaining studied the existence of an association between rhythm perception and reading ability in adults in their L1 and L2 and found no evidence. However, @cason2012rhythmic did find that on-beat sequence primes yielded faster L1 speech processing than off-beat primes in adults through EEG recordings. The contradictory results obtained from these populations suggest that the effects of rhythm in hearing adults may not be as extensive as in deaf individuals in the visual domain. Maybe because hearing individuals do not need to hone their visual perception and processing abilities. Alternatively, there might be some “loss” in the transfer of rhythmic abilities from a visual domain to an auditory domain or vice versa, causing the relationship between rhythm and language in different modalities to be weaker. This conclusion would be supported by findings suggesting that within a modality, the relationship between rhythm and language, in this case speech, is stronger.
Like in tone and pitch, there is evidence that rhythm is associated with language in children, L2 speakers, and atypical populations (beat-deafness, dyslexia and WS) within the auditory modality. Starting with children, @carr2014beat had their typically developing children complete a series of linguistic, verbal memory, rhythmic and musical in general tasks to investigate the relationship between rhythmic synchronization and the encoding of syllable envelops in pre-schoolers. Those children who synchronized better also had better perceptual and cognitive language skills. They were faster at naming objects and color, and their neural encoding of the speech envelope was also better. Encoding of sound is also favored by rhythmic perception in the L2 experience. @cason2019rhythmic and @bhatara2015foreign tested stress placement and rhythmic abilities in L1 French speakers and L2 Spanish. The rhythmic production scores of the participants predicted correct placement of nuclear stress in a L2 lexical stress imitation task; and rhythm perception was actually positively associated with L2 learning experience. These studies suggest that rhythm is indeed related to speech in typical populations within a modality, and specifically, within the auditory domain.
Studies with atypical populations have also yielded positive results supporting a relationship between heard rhythms and speech. @lagrois2019poor tested beat-deaf individuals, that is, individuals who have a documented deficit in tracking musical beat, to analyze how they synchronized to speech. Participants had to tap along sentences in three conditions: regularly spoken, naturally spoken and sung, and were compared to a matched control group. The beat-deaf group showed more variability in its tapping regardless of the condition. The irregularity remained when they tapped to a metronome or at their own pace and was larger than in the typical non-musician control group. These results show that the deficiency in time-keeping mechanisms might underlie both domains rather than being domain-specific. In the same vein, @persici2019rhythmic supported this conclusion when they compared the anticipation abilities of children with developmental dyslexia and typically developing children, and the results showed that children with dyslexia achieved lower scores than their peers in morphosyntactic and rhythmic processing. Therefore, noun prediction based on this information looked alike for both groups, but phonologically- and/or grammatically-based anticipation was hindered for the dyslexic children. Similarly, @sun2018syntactic explored brain responses to syntactic violations in music and language in amusics. Brain responses to incongruities in both domains were reduced, but brain responses at later stages of processing, that is, during semantic processing, were unaffected or showed the same pattern as brain responses in typical individuals to semantic incongruencies.  Based thus on previous findings suggesting the delayed anticipation in rhythm, phonology and morphosyntax, it is not unlikely that structure-based prediction is contingent to a cognitive mechanism common to the auditory mode in general, rather to domain-specific abilities.
Movement is closely tied to music. Moving one or some parts of the body is a natural reaction to a rhythm. Much earlier on, @fraisse1949aptitudes found that humans rock to a musical rhythm as young as one year of age, and we are able to tap along metronome beats by the age of three or four. This sensorimotor behavior is usually based on anticipation of when the next beat is coming, especially at slow tempi [@nozaradan2016individual; @van2015sensorimotor], and the anticipation grows in accuracy with years of musical training [@nozaradan2016individual]. Prediction of oncoming beats is also present in rhythms with tempo changes [@van2015sensorimotor]. The ability to synchronize to beats or changes in rhythm is conditioned by (1) motor limits, (2) the fractured memory of a slow sequence, (3) sensory difficulties at perceiving successiveness at fast tempi [@bartlett1959synchronization], and (4) the complexity of the rhythm [@fraisse2012anticipation]. However, the adaptation is fast, since it can take as few as three consecutive taps accompanying each a consecutive beat in a novel rhythm to achieve a relative simultaneity [@fraisse2012anticipation]. The rhythm structure affects the synchronization pattern, as some tempo speeds are easier to adjust to than others. When the interval between beats is on or under 1500 ms, the synchronization behavior tends towards anticipation of the next beat, while this anticipation rate starts to decrease with beat onset intervals of 1800 ms [@miyake2004two]. From the previous investigations on rhythm, we can see that synchronization is basically prediction of the rhythm timings.
The findings on the relationship between rhythmic abilities and linguistic competence suggest that this relationship might only surface in certain cases, and that transfer from modalities may affect how strong the relationship is. Some of the factors conditioning the existence of a connection between the two domains are atypical development, early stages of cognitive development, nature of L1, or linguistic area in L2 learning. Investigations from other auditory areas, such as speech anticipation, may clarify if and how rhythmic abilities and linguistic abilities are related. Other areas that might be helpful in establishing the nature of the rhythm-language relationship is taking into account how rhythm imbibes from visual and spatial cues. One of the main visual-spatial cues related to rhythm is movement. Movement, like language, is entrenched in our everyday life. We use movement to respond to virtually everything. When driving, we anticipate what other drivers or pedestrians are going to do and adjust accordingly by making foot, manual and head movements. When catching an object in a free fall, we anticipate (more or less successfully) the trajectory and the speed to move our arm and hand fast enough and try to grab the object before it hits the floor.


<!--




# Methods
We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

<!--
## Participants

## Material

## Procedure

## Data analysis
We used `r cite_r("r-references.bib")` for all our analyses.


# Results

# Discussion
-->

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
